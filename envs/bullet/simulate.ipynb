{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time, math\n",
    "import numpy as np\n",
    "import random\n",
    "from enum import Enum\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This section explains the high level APIs of pybullet and how an simulation works in a nutshell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to physics client\n",
    "We will first starting with initiating a connection to the pybullet physics server. This is done using the `connect` function.  We can also set option parameters of the pybullet like gravity, rendering etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a physics client\n",
    "physics_client = p.connect(p.GUI) #P.GUI on computers with display or p.DIRECT for non-graphical version\n",
    "print(\"The pybullet ships some useful data: \", pybullet_data.getDataPath())\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally set the search path for data files. \n",
    "p.setGravity(0,0,-9.8) #set the vector for gravity. This tells the gravity is acting in the negative z direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the world\n",
    "If you have used GUI mode, you would now be seeing a display where axis are drawn. This shows the world is empty. In order to do simulations, we need to add objects to the world. The objects can be loaded as \".urdf\", \".sdf\" or \".mjcf\" formats. These object/robot description files tells the simulator about visual, physical and collision properties of the object/robot to be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we add a plane to the environment. Each object in pybullet will be given a unique id.\n",
    "planeId = p.loadURDF(\"plane.urdf\")\n",
    "print(\"The plane id is: \", planeId)\n",
    "\n",
    "# Now you would be seeing a plane (blue and white grid) in the GUI window. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add a robot to the environment. The user can specify the position and orientation in which the object is to be placed. \n",
    "- The position is in (x,y,z) coordinates. \n",
    "- The PyBullet API uses quaternions to represent orientations. Since quaternions are not very intuitive for people, there are two APIs to convert between quaternions and Euler angles ( getQuaternionFromEuler and getEulerFromQuaternion )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startPos = [0,0,1]\n",
    "startOrientation = p.getQuaternionFromEuler([0,0,0])\n",
    "print(\"The start position of r2d2 is: \", startPos)\n",
    "print(\"The start orientation r2d2 is: \", startOrientation)\n",
    "robotID = p.loadURDF(\"r2d2.urdf\",startPos, startOrientation) #Loads a r2d2 robot at the specified position and orientation\n",
    "# Now you would be seeing a r2d2 robot in the GUI window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a simulation for 1000 steps\n",
    "\n",
    "Since we have added a robot, we can simulate the robot. \n",
    "\n",
    "After you run the cell below, you will find  that the simulation doesn't change the pose of the robot. In fact, the simulation just simulated idle behaviour for 1000 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.resetBasePositionAndOrientation(robotID, startPos, startOrientation)\n",
    "\n",
    "for i in range(1000):\n",
    "    p.stepSimulation()\n",
    "    time.sleep(1./240.) #This will make the simulation run at 240Hz. \n",
    "    #You can change this value to make the simulation run faster or slower. \n",
    "    #The value 1./240. means 1/240 seconds.\n",
    "endPos, endOrn = p.getBasePositionAndOrientation(robotID)\n",
    "print(\"pos after simulation: \",endPos)\n",
    "print(\"orn after simulation: \",endOrn)\n",
    "\n",
    "#once you are done, disconnect the physics client\n",
    "p.disconnect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.getConnectionInfo(physics_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick and Place\n",
    "\n",
    "In previous section, we learnt how pybullet works in a nutshell. Now we move our attention to simulate a simple pick place behaviour. In particular we will be using Franka Emika Panda robot to pick an cube and place it at a desired location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up PyBullet and the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disconnect any physics client running already\n",
    "if p.isConnected():\n",
    "    p.disconnect()\n",
    "\n",
    "# connect to the physics server\n",
    "physics_client = p.connect(p.GUI)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "p.configureDebugVisualizer(p.COV_ENABLE_SHADOWS, 1)\n",
    "p.setGravity(0,0,-9.8)\n",
    "\n",
    "#Load a plane\n",
    "planeId = p.loadURDF(\"plane.urdf\", basePosition = [0,0,0])\n",
    "\n",
    "#Load a table\n",
    "tableId = p.loadURDF(\"table/table.urdf\", basePosition = [0,0,0])\n",
    "# When loaded the table will be placed at the origin. You can camera position using mouse and keyboard (use CTRL + mouse click & drag to rotate) to see the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand the table object\n",
    "\n",
    "print(\"num joints in table: \", p.getNumJoints(tableId)) # get the number of joints in a robot/object. 0 if the object doesn't have any joints.\n",
    "print(\"Body info: \", p.getBodyInfo(tableId)) # get the body info of the object. It outputs name and the urdf file of the object.\n",
    "print(\"Base position and orientation of the table: \", p.getBasePositionAndOrientation(tableId)) # get the base position and orientation of the object.\n",
    "print(\"Base velocity of the table: \", p.getBaseVelocity(tableId)) # get the base velocity of the object.\n",
    "print(\"Base mass of the table: \", p.getDynamicsInfo(tableId, -1)[0]) # get the base mass of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the Table object\n",
    "collision_box = p.getAABB(tableId, -1) # get the axis aligned bounding box of the object.\n",
    "print(\"Collision box of the table: \", collision_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a surprise. The visual table is not the same as the collision box. The collision box only covers the top surface of the table and not the legs. This is because the way the collision is defined in urdf file of the table.  If you read the table.urdf file, it tells you that the table is made up of 5 visual objects. The first one is the top surface and the other four are the legs. The collision is defined as a box only for the top surface\n",
    "\n",
    "```\n",
    "    <collision>\n",
    "      <origin rpy=\"0 0 0\" xyz=\"0 0 0.6\"/>\n",
    "      <geometry>\n",
    "\t \t<box size=\"1.5 1 0.05\"/>\n",
    "      </geometry>\n",
    "    </collision>\n",
    "```\n",
    "\n",
    "So, in that case, how do we get the visual data? p.getVisualShapeData() gives us the info. It return a list where each element in the list represent a visual shape. each visual shape is again a list which tells us the following in order (objectUniqueId, linkIndex, VisualGeometryType, dimensions, meshAssetFileName, localVisualFramePosition, localVisualFrameOrientation, rgbaColor, textureUniqueId) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get visual shape data of table\n",
    "table_visual_data = p.getVisualShapeData(tableId, -1)\n",
    "print(\"Num visual shapes of the table: \", len(table_visual_data))\n",
    "\n",
    "print(\"Visual shape data of the top surface: \", table_visual_data[0]) #The top is present at (0,0,0.6) with dimensions (1.5,1,0.05). \n",
    "                                                                    # This is why the collision box is from (-0.75, -0.5, 0.575) to (0.75, 0.5, 0.625) with 0.001 tolerance-limit.\n",
    "\n",
    "print(\"Visual shape data of the first legs: \", table_visual_data[1]) #The first leg is present at (-0.65,0.4,0.29) with dimensions (0.1, 0.1, 0.58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Panda Robot and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lets add a panda robot on top of the table\n",
    "panda_position = [0,0.5,0.625]\n",
    "panda_orientation = p.getQuaternionFromEuler([0, 0, -math.pi/2])\n",
    "pandaId = p.loadURDF(\"franka_panda/panda.urdf\", panda_position, panda_orientation, useFixedBase = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set simulation parameters. The pybullet allows us to control the speed and time of the simulation. \n",
    "By default the user has to manually execute each step of the simulation using p.stepSimulation().\n",
    "\n",
    "When real-time simulation is enabled, the pybullet uses the internal clock to automatically simulate each simulation step without the need of p.stepSimulation(). However, the user has to be careful in sequencing the timing of actions. For example, the user has to wait untill the robot finsh moving before executing the grasp. If executed at the same time, the fingers will close while the robot is moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS_PER_SECOND = 240\n",
    "TIMESTEP = 1./NUM_STEPS_PER_SECOND\n",
    "USE_REAL_TIME = 0\n",
    "p.setTimeStep(TIMESTEP)\n",
    "p.setRealTimeSimulation(USE_REAL_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets understand panda robot more\n",
    "PANDA_NUM_LINKS = p.getNumJoints(pandaId)\n",
    "print(\"num joints in panda: \", PANDA_NUM_LINKS)\n",
    "\n",
    "#This tells us that the panda urdf file defines 12 links. This might seem unusual because we know that the panda robot is a 7 DOF robot.\n",
    "# The reason is that the urdf file defines some fixed joints that are not actuated just for the purpose of computing forward/inverse kinematics.\n",
    "JointTypes = Enum(\"JointTypes\", [\"JOINT_REVOLUTE\", \"JOINT_PRISMATIC\", \"JOINT_SPHERICAL\", \"JOINT_PLANAR\", \"JOINT_FIXED\"], start= 0)\n",
    "\n",
    "#Get the joint info of the panda robot\n",
    "for i in range(PANDA_NUM_LINKS):\n",
    "    joints_info = p.getJointInfo(pandaId, i)\n",
    "    jname = joints_info[1].decode(\"utf-8\")\n",
    "    jtype = joints_info[2]\n",
    "    print(f\"linkIdx: {i}, \\t Joint name: {jname}, \\t Joint type: {JointTypes(jtype).name}, \\t joint limits: {joints_info[8:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 7 joints are revolute joints and the two joints representing finger is prismatic joint. Other joints are fixed joints.\n",
    "PANDA_NUM_DOF = 7\n",
    "PANDA_END_EFFECTOR_INDEX = 11 #Some people might use 8 as the end effector index.\n",
    "PANDA_REVOLUTE_JOINTS = [0,1,2,3,4,5,6]\n",
    "PANDA_PRISMATIC_JOINTS = [9,10]\n",
    "PANDA_FIXED_JOINTS = [7,8,11]\n",
    "\n",
    "PANDA_HOME_POSITION = [0, -0.785, 0, -2.356, 0, 1.571, 0.785, 0.04] #joint angles in radians for the panda home position. These values can be obtained from the Moveit ROS or Franka Control Inteface.\n",
    "joint_lower_limts = [-2.8973, -1.7628, -2.8973, -3.0718, -2.8973, -0.0175, -2.8973] #joint lower limits in radians. Can be found in https://frankaemika.github.io/docs/control_parameters.html\n",
    "joint_upper_limits = [2.8973, 1.7628, 2.8973, -0.0698, 2.8973, 3.7525, 2.8973] #joint upper limits in radians\n",
    "joint_ranges = jr = [7]*PANDA_NUM_DOF #joint ranges for null space (@TODO: set them to proper range)\n",
    "\n",
    "PANDA_FINGER_OPEN = 0.04\n",
    "PANDA_FINGER_CLOSED = 0\n",
    "\n",
    "#Note that the two finger may not move independent. Those are coupled.\n",
    "#create a constraint to keep the fingers centered\n",
    "finger_constraint = p.createConstraint(\n",
    "\t\t\tpandaId, 9, pandaId, 10,\n",
    "\t\t\tjointType=p.JOINT_GEAR,\n",
    "\t\t\tjointAxis=[1, 0, 0],\n",
    "\t\t\tparentFramePosition=[0, 0, 0],\n",
    "\t\t\tchildFramePosition=[0, 0, 0])\n",
    "\n",
    "p.changeConstraint(finger_constraint, gearRatio=-1, erp=0.1, maxForce=50) #These values are taken from the pybullet examples https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_robots/panda/panda_sim_grasp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Bring panda to its home position\n",
    "Now we make the robot move to the given joint positions. \n",
    "The function p.setJointMotorControl2 is used to control the robot joints. You can control using POSITION_CONTROL, VELOCITY_CONTROL, TORQUE_CONTROL, etc.\n",
    "Here we are using POSITION_CONTROL to control the robot joints. In particular, we are setting the target position of each joints to the given joint_positions.\n",
    "'''\n",
    "def is_panda_moving():\n",
    "    #Panda is moving if any of the joint velocities is not close to zero\n",
    "    for j in PANDA_REVOLUTE_JOINTS:\n",
    "        state = p.getJointState(pandaId, j)\n",
    "        this_joint_velocity = state[1]\n",
    "        if not math.isclose(this_joint_velocity, 0, abs_tol=1e-5):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def panda_execute_action(max_steps = 5000):\n",
    "    #manually simulate the each step of the simulation to make the robot move to the home position\n",
    "    #we define a function is_panda_moving to check if panda is in a moving state. \n",
    "    #If the panda is moving, we keep simulating the environment. Once the panda stops moving, we stop the simulation.  \n",
    "    p.stepSimulation() #Initiate the first simulation step\n",
    "    step, status = 0, True\n",
    "    while is_panda_moving() and step < max_steps:\n",
    "        p.stepSimulation()\n",
    "        time.sleep(TIMESTEP)\n",
    "        step+=1\n",
    "        if step == 2400:\n",
    "            status = False\n",
    "    return status\n",
    "\n",
    "def panda_move_to_home_position(open_fingers:bool = True):\n",
    "    index =0 \n",
    "    for j in range(p.getNumJoints(pandaId)):\n",
    "        p.changeDynamics(pandaId, j, linearDamping=0, angularDamping=0)\n",
    "        info = p.getJointInfo(pandaId, j)\n",
    "        jointName = info[1]\n",
    "        jointType = info[2]\n",
    "        if jointType == p.JOINT_REVOLUTE:           \n",
    "            p.setJointMotorControl2(bodyIndex = pandaId, jointIndex=index, controlMode = p.POSITION_CONTROL, targetPosition = PANDA_HOME_POSITION[index], maxVelocity = 0.6)\n",
    "            index=index+1\n",
    "\n",
    "    #Open the gripper\n",
    "    if open_fingers:\n",
    "        for i in PANDA_PRISMATIC_JOINTS:\n",
    "            p.setJointMotorControl2(pandaId, i, p.POSITION_CONTROL, targetPosition=PANDA_FINGER_OPEN, force=100)\n",
    "    status = panda_execute_action()\n",
    "    if not status:\n",
    "        print(\"Warning: Action panda_move_to_home_position is taking long time.\")\n",
    "    \n",
    "            \n",
    "panda_move_to_home_position()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define helper functions to move the panda robot to a given joint position depending on the concepts learnt so far.\n",
    "\n",
    "def panda_move_to_position(joint_positions:list, include_fingers:bool = False):\n",
    "    if include_fingers:\n",
    "        assert len(joint_positions) == PANDA_NUM_DOF + 2, f\"The joint_positions should be of length {PANDA_NUM_DOF + 2} if include_fingers is True\"\n",
    "    else:\n",
    "        assert len(joint_positions) == PANDA_NUM_DOF, f\"The joint_positions should be of length {PANDA_NUM_DOF} if include_fingers is False\"\n",
    "    \n",
    "    index = 0\n",
    "    for j in range(p.getNumJoints(pandaId)):\n",
    "        info = p.getJointInfo(pandaId, j)\n",
    "        jointType = info[2]\n",
    "        if jointType == p.JOINT_REVOLUTE:\n",
    "            p.setJointMotorControl2(bodyIndex = pandaId, jointIndex=index, controlMode = p.POSITION_CONTROL, targetPosition = joint_positions[index], maxVelocity = 0.6 )\n",
    "            index=index+1\n",
    "    status = panda_execute_action()\n",
    "    if include_fingers:\n",
    "        p.setJointMotorControl2(pandaId, 9, p.POSITION_CONTROL, targetPosition=joint_positions[-2], force=100)\n",
    "        p.setJointMotorControl2(pandaId, 10, p.POSITION_CONTROL, targetPosition=joint_positions[-1], force=100) \n",
    "    if not status:\n",
    "        print(\"Warning: Action panda_move_to_position is taking long time.\")\n",
    "\n",
    "def panda_fingers_open():\n",
    "    p.setJointMotorControl2(pandaId, 9, p.POSITION_CONTROL, targetPosition=PANDA_FINGER_OPEN, force=100)\n",
    "    p.setJointMotorControl2(pandaId, 10, p.POSITION_CONTROL, targetPosition=PANDA_FINGER_OPEN, force=100)\n",
    "    panda_execute_action(max_steps = 50)\n",
    "\n",
    "def panda_fingers_close():\n",
    "    p.setJointMotorControl2(pandaId, 9, p.POSITION_CONTROL, targetPosition=PANDA_FINGER_CLOSED, force=100)\n",
    "    p.setJointMotorControl2(pandaId, 10, p.POSITION_CONTROL, targetPosition=PANDA_FINGER_CLOSED, force=100)\n",
    "    panda_execute_action(max_steps = 50)\n",
    "\n",
    "def simulate_world(num_steps):\n",
    "    for _ in range(num_steps):\n",
    "        p.stepSimulation()\n",
    "        time.sleep(TIMESTEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets us add a cube on the table\n",
    "cube_position = [0,0,0.63]\n",
    "cube_orientation = p.getQuaternionFromEuler([random.uniform(-math.pi/2, math.pi/2), random.uniform(-math.pi/2, math.pi/2), random.uniform(-math.pi/2, math.pi/2)]) #random cube orientation\n",
    "cubeId = p.loadURDF(\"cube_small.urdf\", cube_position, cube_orientation)\n",
    "\n",
    "#If you see the cube is inside the table especially if the realtime simulation is off. When simulated, the cube will automatically come out of the table.\n",
    "#Hence, we simulate the environment for 1 sec to let the cube come out of the table.\n",
    "if not USE_REAL_TIME:\n",
    "    simulate_world(num_steps = NUM_STEPS_PER_SECOND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking up the cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panda_pick_object(position, orientation = None):\n",
    "    assert orientation is None, \"The orientation is not implemented yet.\"\n",
    "    #restposes for null space\n",
    "    rp = PANDA_HOME_POSITION\n",
    "\n",
    "    #We will use the inverse kinematics to move the panda end effector to the cube position.\n",
    "    #The function p.calculateInverseKinematics is used to calculate the inverse kinematics. The function returns the joint positions which will move the panda end effector to the cube position.\n",
    "    joint_positions = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX, targetPosition=position, lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, \n",
    "                                                jointRanges=joint_ranges, restPoses=rp, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "\n",
    "    # Note that we just use targetPosition. We can also optionaly give targetOrientation. \n",
    "    # However, the targetOrientation is not same as the cube orientation because we have arbitrarity rotated the cube.\n",
    "\n",
    "    #Move to home position, move to the joint_positions found by inverse kinematics and close the fingers to grasp the object\n",
    "    panda_move_to_home_position(open_fingers=True)\n",
    "    panda_move_to_position(joint_positions, include_fingers=True)\n",
    "    panda_fingers_close()\n",
    "    panda_move_to_home_position(open_fingers=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we get the position and orientation of the cube\n",
    "cube_position, cube_orientation = p.getBasePositionAndOrientation(cubeId)\n",
    "print(\"Cube position: \", cube_position)\n",
    "print(\"Cube orientation: \", cube_orientation)\n",
    "\n",
    "# Lets move the panda End effector to the cube position and pick object\n",
    "panda_pick_object(cube_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place the object at desired position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panda_place_at_target_position(position, orientation = None):\n",
    "    assert orientation is None, \"The target_cube_orientation is not implemented yet.\"\n",
    "    \n",
    "    rp = PANDA_HOME_POSITION\n",
    "    #We will use the inverse kinematics to move the panda end effector to the target_cube position.\n",
    "    joint_positions = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX, targetPosition=position,lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, \n",
    "                                               jointRanges=joint_ranges, restPoses=rp, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "\n",
    "    panda_move_to_position(joint_positions[0:7], include_fingers=False)\n",
    "    panda_fingers_open()\n",
    "    panda_move_to_home_position(open_fingers=False)\n",
    "\n",
    "\n",
    "#Now Let us place the cube at the desired position\n",
    "target_cube_position = [-0.3, 0, 0.66]\n",
    "target_cube_orientation = p.getQuaternionFromEuler([0,0,0])\n",
    "\n",
    "panda_place_at_target_position(target_cube_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that due to multiple reasons, the cube may not be placed at the exact position. The reasons include the inaccuracies in the inverse kinematics, the inaccuracies in the simulation, etc.\n",
    "The cube may fall down or the arm may collide with the object while moving to home position. Now, we will implement a simple recovery to handle such cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Recovery\n",
    "error_recovery_attempts = 3\n",
    "for i in range(error_recovery_attempts):\n",
    "    current_cube_position, _ = p.getBasePositionAndOrientation(cubeId)\n",
    "    if np.linalg.norm(np.array(current_cube_position) - np.array(target_cube_position)) < 0.01:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Error in placing the object. Retrying count {i}...\")\n",
    "        panda_pick_object(current_cube_position)\n",
    "        panda_place_at_target_position(target_cube_position)\n",
    "\n",
    "current_cube_position, _ = p.getBasePositionAndOrientation(cubeId)\n",
    "if np.linalg.norm(np.array(current_cube_position) - np.array(target_cube_position)) < 0.01:\n",
    "    print(\"Object placed successfully.\")\n",
    "else:\n",
    "    print(f\"Error in placing the object even after trying to recover for {error_recovery_attempts} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick and place through Crane grasphing\n",
    "\n",
    "We can see that the robot hit/collide with object while moving to home position. We can handle this by performing a crane grasp. In this method, the robot moves to a position above the object and then moves down to grasp the object and vice versa. This way, the robot doesn't collide with the object while moving to home position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement crane grasping and releasing\n",
    "\n",
    "def panda_crane_pick_object(position, orientation = None):\n",
    "    assert orientation is None, \"The orientation is not implemented yet.\"\n",
    "    #restposes for null space\n",
    "    rp = PANDA_HOME_POSITION\n",
    "\n",
    "    #We will use the inverse kinematics to move the panda end effector to the cube position.\n",
    "    #The function p.calculateInverseKinematics is used to calculate the inverse kinematics. The function returns the joint positions which will move the panda end effector to the cube position.\n",
    "    move_up_joint_positions = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX,  targetPosition=[position[0], position[1], position[2]+0.09], targetOrientation = [0, math.pi, 0], \n",
    "                                                   lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, \n",
    "                                                jointRanges=joint_ranges, restPoses=rp, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "    \n",
    "    grasp_position = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX,  targetPosition = position, targetOrientation = [0, math.pi, 0], \n",
    "                                                   lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, \n",
    "                                                jointRanges=joint_ranges, restPoses=rp, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "\n",
    "    # Note that we just use targetPosition. We can also optionaly give targetOrientation. \n",
    "    # However, the targetOrientation is not same as the cube orientation because we have arbitrarity rotated the cube.\n",
    "\n",
    "    #Move to home position, move to the joint_positions found by inverse kinematics and close the fingers to grasp the object\n",
    "    panda_move_to_home_position(open_fingers=True)\n",
    "    panda_move_to_position(move_up_joint_positions, include_fingers=True)\n",
    "    panda_move_to_position(grasp_position, include_fingers=True)\n",
    "    panda_fingers_close()\n",
    "    panda_move_to_home_position(open_fingers=False)\n",
    "\n",
    "\n",
    "def panda_crane_place_at_target_position(position, orientation = None):\n",
    "    assert orientation is None, \"The orientation is not implemented yet.\"\n",
    "    #restposes for null space\n",
    "    rp = PANDA_HOME_POSITION\n",
    "\n",
    "    #We will use the inverse kinematics to move the panda end effector to the cube position.\n",
    "    #The function p.calculateInverseKinematics is used to calculate the inverse kinematics. The function returns the joint positions which will move the panda end effector to the cube position.\n",
    "    move_up_joint_positions = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX,  targetPosition=[position[0], position[1], position[2]+0.09], targetOrientation = [0, math.pi, 0], \n",
    "                                                   lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, \n",
    "                                                jointRanges=joint_ranges, restPoses=rp, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "    \n",
    "    place_position = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX,  targetPosition = position, targetOrientation = [0, math.pi, 0], \n",
    "                                                   lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, \n",
    "                                                jointRanges=joint_ranges, restPoses=rp, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "\n",
    "    # Note that we just use targetPosition. We can also optionaly give targetOrientation. \n",
    "    # However, the targetOrientation is not same as the cube orientation because we have arbitrarity rotated the cube.\n",
    "\n",
    "    #Move to home position, move to the joint_positions found by inverse kinematics and close the fingers to grasp the object\n",
    "    panda_move_to_position(move_up_joint_positions, include_fingers=True)\n",
    "    panda_move_to_position(place_position, include_fingers=True)\n",
    "    panda_fingers_open()\n",
    "    panda_move_to_position(move_up_joint_positions, include_fingers=True)\n",
    "    panda_move_to_home_position(open_fingers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we get the position and orientation of the cube\n",
    "cube_position, cube_orientation = p.getBasePositionAndOrientation(cubeId)\n",
    "print(\"Cube position: \", cube_position)\n",
    "print(\"Cube orientation: \", cube_orientation)\n",
    "\n",
    "# Lets move the panda End effector to the cube position and pick object\n",
    "panda_crane_pick_object(cube_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Let us place the cube at the desired position\n",
    "target_cube_position = [-0.3, 0, 0.66]\n",
    "target_cube_orientation = p.getQuaternionFromEuler([0,0,0])\n",
    "\n",
    "panda_crane_place_at_target_position(target_cube_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cube_position, _ = p.getBasePositionAndOrientation(cubeId)\n",
    "print(current_cube_position)\n",
    "if np.linalg.norm(np.array(current_cube_position) - np.array(target_cube_position)) < 0.05:\n",
    "    print(\"Object placed successfully.\")\n",
    "else:\n",
    "    print(f\"Error in placing the object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering Images and PointClouds\n",
    "\n",
    "The previous section described how to use inverse kinematics and joint control to perform a simple pick and place task. In this section, we will learn how to render images and point clouds from the simulation. The pybullet allows us to render RGB image, depth and segmentation masks using a synthetic camera. The user can specify the camera parameters such as field of view, aspect ratio, near and far clipping planes, etc. First, we will look into understanding the camera parameter and set up a synthetic camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Camera Parameters\n",
    "The pybullet uses OpenGL to render the images. The OpenGL uses a camera model which is different from the pinhole camera model. The OpenGL camera model is defined by the following parameters:\n",
    "1) **View Matrix**: The view matrix defines the position and orientation of the camera in the world coordinates. The view matrix is defined by the eye position, target position and the up vector. The eye position is the position of the camera in the world coordinates. The target position is the point where the camera is looking at. The up vector is the direction of the top of the camera.\n",
    "    - **eyePosition**: The position of the camera in the world coordinates\n",
    "    - **targetPosition**: The position of the target point in the world coordinates\n",
    "    - **upVector**: The up vector of the camera\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/pybullet_camera.png\" alt=\"Alt text\" title=\"a title\">\n",
    "</p>\n",
    "\n",
    "2) **Projection Matrix**: The projection matrix defines the camera intrinsics. Pybullet provides two types of projection matrices: perspective and orthographic. The perspective projection matrix is defined by the field of view, aspect ratio, near and far clipping planes. The orthographic projection matrix is defined by the left, right, bottom, top, near and far clipping planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function shows an example of how to render the desired camera view of the panda robot.\n",
    "def rgba2rgb(rgba, background=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    Convert rgba to rgb.\n",
    "\n",
    "    Args:\n",
    "        rgba (tuple):\n",
    "        background (tuple):\n",
    "\n",
    "    Returns:\n",
    "        rgb (tuple):\n",
    "    \"\"\"\n",
    "    row, col, ch = rgba.shape\n",
    "    if ch == 3:\n",
    "        return rgba\n",
    "    assert ch == 4, 'RGBA image has 4 channels.'\n",
    "\n",
    "    rgb = np.zeros((row, col, 3), dtype='float32')\n",
    "    r, g, b, a = rgba[:, :, 0], rgba[:, :, 1], rgba[:, :, 2], rgba[:, :, 3]\n",
    "    a = np.asarray(a, dtype='float32') / 255.0\n",
    "    R, G, B = background\n",
    "    rgb[:, :, 0] = r * a + (1.0 - a) * R\n",
    "    rgb[:, :, 1] = g * a + (1.0 - a) * G\n",
    "    rgb[:, :, 2] = b * a + (1.0 - a) * B\n",
    "    return np.asarray(rgb, dtype='uint8')\n",
    "\n",
    "\n",
    "def get_camera_image(camera_view:str, width = 1080, height = 1080, return_cam_parameters = False):\n",
    "    '''\n",
    "    Input: camera_view: str: The desired camera view. It can be 'top_down', 'front_perspective' or 'eye-in-hand'\n",
    "    \n",
    "    '''\n",
    "    if camera_view == 'top_down':\n",
    "        nearVal = 0.1\n",
    "        farVal = 0.5\n",
    "        #Rendering Top-Down View\n",
    "        view_matrix = p.computeViewMatrix(cameraEyePosition=[0,0,0.8], cameraTargetPosition=[0,0,0.64], cameraUpVector=[0,0.1,-1], ) #Top-Down view\n",
    "        projection_matrix = p.computeProjectionMatrix(-0.5, .5, -0.3, 0.3, nearVal, farVal)\n",
    "\n",
    "    elif camera_view == 'front_perspective':\n",
    "        #Rendering Front Perspective View\n",
    "        nearVal = 0.1\n",
    "        farVal = 3\n",
    "        view_matrix = p.computeViewMatrix(cameraEyePosition=[0.9,0,1.2], cameraTargetPosition=[0,0,0.64], cameraUpVector=[0,0,1], )\n",
    "        projection_matrix = p.computeProjectionMatrix(-0.08, 0.08, -0.1, 0.1, nearVal, farVal)\n",
    "\n",
    "    elif camera_view == 'eye-in-hand':\n",
    "        nearVal = 0.03\n",
    "        farVal = 2\n",
    "        #Rendering Eye-in-Hand View\n",
    "        hand_position, hand_orientation, _ , _ , _ , _  = p.getLinkState(pandaId, 8, computeForwardKinematics=True)\n",
    "        rot_matrix = p.getMatrixFromQuaternion(hand_orientation)\n",
    "        rot_matrix = np.array(rot_matrix).reshape(3,3)\n",
    "        \n",
    "        #The camera is placed at the end effector of the panda robot. we find the camera-vector which defines the direction in which the camera is looking.\n",
    "        init_camera_vector = np.array([0,0,1]) #z-axis\n",
    "        camera_vector = np.dot(rot_matrix, init_camera_vector)\n",
    "\n",
    "        #Note up vector is orthogonal to the camera vector. We can choose y-axix as the init_camera_vector is z-axis.\n",
    "        init_up_vector = np.array([0,1,0]) #y-axis\n",
    "        up_vector = np.dot(rot_matrix, init_up_vector)\n",
    "        \n",
    "        cam_target_pos = np.array(hand_position) + np.array([0,0,-0.02]) #[0,0,0.2] is added to the hand position to move the camera slightly below the hand.\n",
    "        \n",
    "        view_matrix = p.computeViewMatrix(cameraEyePosition= cam_target_pos, cameraTargetPosition=cam_target_pos+0.1*camera_vector, cameraUpVector = up_vector)\n",
    "        \n",
    "        #Adjust FOV and aspect if you want the gripper to be visible\n",
    "        projection_matrix = p.computeProjectionMatrixFOV(fov=90, aspect=1, nearVal=nearVal, farVal=farVal) #FOV is used to set the field of view of the camera so that the view looks natural. We could use computeProjectionMatrix also but it may look unnatural. \n",
    "\n",
    "    #get the camera image\n",
    "    _, _, rgbaImg, depthImg, segImg = p.getCameraImage(width=width, height=height, viewMatrix=view_matrix, projectionMatrix=projection_matrix, lightDirection = [0,0,1], shadow = 0, renderer=p.ER_TINY_RENDERER)\n",
    "\n",
    "    #convert rgba to rgb\n",
    "    rgbImg = rgba2rgb(rgbaImg)\n",
    "    \n",
    "    #convert depthBuffer to depth image using near and far values. See https://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/ and https://stackoverflow.com/questions/70955660/how-to-get-depth-images-from-the-camera-in-pybullet \n",
    "    depthImg = farVal * nearVal / (farVal - (farVal - nearVal) * depthImg)\n",
    "    \n",
    "    if return_cam_parameters:\n",
    "        return rgbaImg, depthImg, segImg, projection_matrix, view_matrix\n",
    "    \n",
    "    return rgbImg, depthImg, segImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye-in-hand camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the image, depth and mask\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def visualize(rgbImg, depthImg, segImg):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(rgbImg)\n",
    "    plt.title(\"Camera Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(depthImg)\n",
    "    plt.title(\"Depth Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(segImg)\n",
    "    plt.title(\"Mask Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "panda_move_to_home_position(open_fingers=False)\n",
    "target_position = p.getBasePositionAndOrientation(cubeId)[0]\n",
    "joint_positions = p.calculateInverseKinematics(pandaId, PANDA_END_EFFECTOR_INDEX, targetPosition=target_position, lowerLimits=joint_lower_limts, upperLimits=joint_upper_limits, jointRanges=joint_ranges, \n",
    "                                               restPoses=PANDA_HOME_POSITION, maxNumIterations=10000, residualThreshold=1e-4)\n",
    "\n",
    "for i in range(7):\n",
    "    p.setJointMotorControl2(pandaId, i, p.POSITION_CONTROL, targetPosition=joint_positions[i], maxVelocity=1)\n",
    "\n",
    "p.stepSimulation()\n",
    "index = 0\n",
    "while is_panda_moving():\n",
    "    p.stepSimulation()\n",
    "    if index % 10 == 0:\n",
    "        rgbImg, depthImg, segImg = get_camera_image('eye-in-hand')\n",
    "    index += 1\n",
    "    # visualize(rgbImg, depthImg, segImg)\n",
    "    time.sleep(TIMESTEP)\n",
    "    \n",
    "#Do you observe the cube disappers when the end effector reaches the cube position. \n",
    "# This is because the synthetic camera is placed in end-effector enters into the cube as the end-effector reaches the cube. \n",
    "# You can modify the camera parameters and experiment with it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Clouds\n",
    "The openGL camera model that is provided in pybullet outputs a non-linear depth buffer. We can convert the depth buffer into normal depth in camera frame using the following formula:\n",
    "$$ depthImg = farVal * nearVal / (farVal - (farVal - nearVal) * depthImg) $$\n",
    "\n",
    "Once, we RGB and Depth image, we can convert the depth image into point cloud as follows:\n",
    "\n",
    "1. Get the camera intrinsics from projection matrix. Let the intrinsics be $K = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "2. Get the camera extrinsics from view matrix.\n",
    "3. For each pixel in the depth image [x,y,d], calculate the 3D point in camera frame using the following formula:\n",
    "$$ Z = d $$\n",
    "$$ X = (x - c_x) * d / f_x $$\n",
    "$$ Y = (y - c_y) * d / f_y $$\n",
    "4. If required, finally transform the 3D point from camera frame to world frame.\n",
    "\n",
    "---\n",
    "\n",
    "**Getting Camera Intrinsics and Extrinsics**: \n",
    "\n",
    "Now, we will explain in detail how to get the camera instrincs from the projection matrix and how to get the camera extrinsics from the view matrix. pybullet got two functions to construct the projection matrix: ```computeProjectionMatrixFOV(fov, aspect, near, far)``` and ```computeProjectionMatrix(left, right, bottom, top, near, far)```. Converting from FOV params to the other group is done as follows: \n",
    "```python\n",
    "top    = tan(fov/2)*near\n",
    "bottom = -top\n",
    "right  = top*aspect\n",
    "left   = -right\n",
    "# aspect = width/height\n",
    "```\n",
    "The code below test this conversion. Now we will see how to get the camera instrincs from the projection matrix. \n",
    "\n",
    "Let $n=near, f=far, r=right, l=left, t=top, b=bottom$. Then, the pybullet constructs the projection matrix using the following formula:\n",
    "$$ \\begin{bmatrix} \\frac{2n}{r-l} & 0 & \\frac{r+l}{r-l} & 0 \\\\ 0 & \\frac{2n}{t-b} & \\frac{t+b}{t-b} & 0 \\\\ 0 & 0 & -\\frac{f+n}{f-n} & -\\frac{2fn}{f-n} \\\\ 0 & 0 & -1 & 0 \\end{bmatrix} $$\n",
    "\n",
    "So in terms of FOV (in radians) and aspect the projection matrix is\n",
    "$$ \\begin{bmatrix} \\frac{1}{tan(fov/2)*aspect} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{tan(fov/2)} & 0 & 0 \\\\ 0 & 0 & -\\frac{f+n}{f-n} & -\\frac{2fn}{f-n} \\\\ 0 & 0 & -1 & 0 \\end{bmatrix} $$\n",
    "\n",
    "It is also important to note that the pybullet uses Normalized Device Coordinates (NDC) for the depth buffer. In particular the NDC is defined as follows:\n",
    "$$ \\begin{bmatrix} \\frac{2}{width} & 0 & 0 & -frac{r+l}{r-l}\\\\ 0 & \\frac{2}{height} & 0 & -\\frac{t+b}{t-b} \\\\ 0 & 0 & -\\frac{2}{far-near} & -\\frac{far+near}{far-near} \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "The projection matrix given by pybullet is equal to NDC multiplied by the perspective matrix given by $\\begin{bmatrix} fx & 0 & -cx 0 \\\\ 0 & fy & -cy 0 \\\\ 0 & 0 & n+f & n*f \\\\ 0 & 0 & -1 & 0 \\end{bmatrix}$\n",
    "\n",
    "So, the intrinsics are $K = \\begin{bmatrix} \\frac{width}{2*aspect*tan(fov/2)} & 0 & \\frac{width}{2} \\\\ 0 & \\frac{height}{2*tan(fov/2)} & \\frac{height}{2} \\\\ 0 & 0 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code for checking if the conversion from FOV and Aspect to left, right, bottom, top is correct.\n",
    "\n",
    "# fov = 60\n",
    "# width = 320\n",
    "# height = 240\n",
    "# aspect = width / height\n",
    "# near = 0.2\n",
    "# far = 10\n",
    "\n",
    "# projectionMatrix = p.computeProjectionMatrixFOV(fov, aspect, near, far)\n",
    "# print(projectionMatrix)\n",
    "\n",
    "# top = np.tan(np.deg2rad(fov)/2)*near\n",
    "# bottom = -top\n",
    "# right = top * aspect\n",
    "# left = -right\n",
    "\n",
    "# projectionMatrix = p.computeProjectionMatrix(left, right, bottom, top, near, far)\n",
    "# print(projectionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is adopted from stackoverflow https://stackoverflow.com/questions/60430958/understanding-the-view-and-projection-matrix-from-pybullet\n",
    "# The author didn't check the correctness of the code in detail but it seems to be correct. \n",
    "# However, we will be using open3d library to create and visualize the point cloud.\n",
    "\n",
    "def get_intrinsics(fov, aspect, width, height):\n",
    "    '''\n",
    "    fov: float: field of view in degrees\n",
    "    aspect: float: aspect ratio of the camera\n",
    "    width: int: width of the image\n",
    "    height: int: height of the image\n",
    "    '''\n",
    "    fx = width / (2 * aspect * np.tan(np.radians(fov / 2)))\n",
    "    fy = height / (2 * np.tan(np.radians(fov / 2)))\n",
    "    cx = width / 2\n",
    "    cy = height / 2\n",
    "    return fx, fy, cx, cy\n",
    "\n",
    "def get_instrinsics_from_projection_matrix(projection_matrix, width, height):\n",
    "    '''\n",
    "    projection_matrix: list: projection matrix of the camera\n",
    "    width: int: width of the image\n",
    "    height: int: height of the image\n",
    "    '''\n",
    "    fx = projection_matrix[0] * width / 2\n",
    "    fy = projection_matrix[5] * height / 2\n",
    "    cx = projection_matrix[2] * width / 2 + width / 2\n",
    "    cy = projection_matrix[6] * height / 2 + height / 2\n",
    "    return fx, fy, cx, cy\n",
    "\n",
    "def get_extrinsics(view_matrix):\n",
    "    Tc = np.array([[1,  0,  0,  0],\n",
    "                   [0,  -1,  0,  0],\n",
    "                   [0,  0,  -1,  0],\n",
    "                   [0,  0,  0,  1]]).reshape(4,4)\n",
    "    T = np.linalg.inv(view_matrix) @ Tc\n",
    "\n",
    "    return T\n",
    "\n",
    "def get_transformed_pointcloud(depth_image, fx, fy, cx, cy, extrinsics_matrix, max_depth=5.0):\n",
    "    # Get the point cloud\n",
    "    depth_image = np.array(depth_image)\n",
    "    depth_image = depth_image.astype(np.float32)\n",
    "    height, width = depth_image.shape\n",
    "    x = np.arange(0, width).reshape(1, -1).repeat(height, axis=0)\n",
    "    y = np.arange(0, height).reshape(-1, 1).repeat(width, axis=1)\n",
    "    z = depth_image\n",
    "    x = (x - cx) * z / fx\n",
    "    y = (y - cy) * z / fy\n",
    "    pointcloud = np.stack([x, y, z], axis=-1)\n",
    "    pointcloud = pointcloud.reshape(-1, 3)\n",
    "    \n",
    "    # filter out the points with depth > max_depth\n",
    "    pointcloud = pointcloud[pointcloud[:, 2] < max_depth]\n",
    "    \n",
    "    pointcloud = np.concatenate([pointcloud, np.ones((pointcloud.shape[0], 1))], axis=-1)\n",
    "    pointcloud = np.dot(extrinsics_matrix, pointcloud.T).T\n",
    "    pointcloud = pointcloud[:, :3]\n",
    "    return pointcloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions to access the camera intrinsics and extrinsics\n",
    "\n",
    "class CameraIntrinsics(object):\n",
    "    def __init__(self, fx, fy, cx, cy, width, height):\n",
    "        self.fx = fx\n",
    "        self.fy = fy\n",
    "        self.cx = cx\n",
    "        self.cy = cy\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    \n",
    "    def get_intrinsics_matrix(self):\n",
    "        return np.array([[self.fx, 0, self.cx],\n",
    "                         [0, self.fy, self.cy],\n",
    "                         [0, 0, 1]])\n",
    "    def get_fov(self):\n",
    "        return 2 * np.arctan(self.width / (2 * self.fx))\n",
    "    \n",
    "    def get_aspect_ratio(self):\n",
    "        return self.width / self.height\n",
    "\n",
    "    def from_fov(self, fov, aspect, width, height):\n",
    "        self.fx, self.fy, self.cx, self.cy = get_intrinsics(fov, aspect, width, height)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        return self\n",
    "    \n",
    "class CameraExtrinsics(object):\n",
    "    def __init__(self, extrinsics_matrix = None):\n",
    "        if extrinsics_matrix is not None and not isinstance(extrinsics_matrix, np.ndarray):\n",
    "            extrinsics_matrix = np.array(extrinsics_matrix).reshape(4,4)\n",
    "        self.extrinsics_matrix = extrinsics_matrix\n",
    "    \n",
    "    def get_extrinsics_matrix(self):\n",
    "        return self.extrinsics_matrix\n",
    "\n",
    "    def from_view_matrix(self, view_matrix):\n",
    "        if not isinstance(view_matrix, np.ndarray):\n",
    "            view_matrix = np.array(view_matrix).reshape(4,4)\n",
    "        #open 3d uses the camera coordinate system where z-axis is pointing in the opposite direction of the camera.\n",
    "        #Thus, we need to flip the z-axis to make it consistent with the open3d coordinate system.\n",
    "        Tc = np.array([[1,  0,  0,  0],\n",
    "                       [0,  -1,  0,  0],\n",
    "                       [0,  0,  -1,  0],\n",
    "                       [0,  0,  0,  1]]).reshape(4,4) \n",
    "        mat = np.linalg.inv(view_matrix.T) @ Tc #TODO: Check if the transpose is required\n",
    "        self.extrinsics_matrix = mat\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now use open3d library to create and visualize the point cloud.\n",
    "import open3d as o3d\n",
    "\n",
    "def visualize_pointcloud(pcd):\n",
    "    if isinstance(pcd, o3d.geometry.PointCloud) or isinstance(pcd, o3d.t.geometry.PointCloud):\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "    elif isinstance(pcd, list):\n",
    "        o3d.visualization.draw_geometries(pcd)\n",
    "    \n",
    "\n",
    "def o3d_create_pointcloud(rgbImg, depthImg, segImg, camera_intrinsics:CameraIntrinsics, camera_extrinsics:CameraExtrinsics = None):\n",
    "    device = o3d.core.Device(\"CPU:0\")\n",
    "    dtype = o3d.core.float32\n",
    "\n",
    "    intrinsic = o3d.core.Tensor([[camera_intrinsics.fx, 0, camera_intrinsics.cx],\n",
    "                                [0, camera_intrinsics.fy, camera_intrinsics.cy],\n",
    "                                [0, 0, 1]], dtype, device)\n",
    "    \n",
    "    if camera_extrinsics is None:\n",
    "        extrinsic = o3d.core.Tensor(np.eye(4), dtype, device)\n",
    "    else:\n",
    "        extrinsic = camera_extrinsics.get_extrinsics_matrix()\n",
    "        \n",
    "    rgbd = o3d.t.geometry.RGBDImage(o3d.t.geometry.Image(rgbImg.astype(\"float32\")/255), o3d.t.geometry.Image(depthImg.astype(\"float32\")))\n",
    "    \n",
    "    pcd = o3d.t.geometry.PointCloud.create_from_rgbd_image(rgbd, intrinsic, extrinsic, depth_scale = 1.0 )\n",
    "    return pcd.to_legacy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add additional objects\n",
    "#Lets add a mug on the table\n",
    "cylinder_position = [0.3,0,0.63]\n",
    "cylinder_orientation = p.getQuaternionFromEuler([0,0,0])\n",
    "mugId = p.loadURDF(\"urdf/mug.urdf\", cylinder_position, cylinder_orientation)\n",
    "\n",
    "#add Lego to table\n",
    "banana_position = [0.3,0.4,0.63]\n",
    "banana_orientation = p.getQuaternionFromEuler([0,0,0])\n",
    "legoId = p.loadURDF(\"lego/lego.urdf\", banana_position, banana_orientation, globalScaling=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging two point clouds\n",
    "Now we will render images of mug from two different camera views and merge the point clouds to get a single point cloud. We asume that the camera intrinsics and extrinsics are known and **are not noisy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tc = np.array([[1,  0,  0,  0],\n",
    "                [0,  -1,  0,  0],\n",
    "                [0,  0,  -1,  0],\n",
    "                [0,  0,  0,  1]]).reshape(4,4) \n",
    "\n",
    "#Get the camera image from top-down view\n",
    "width= 1080\n",
    "height = 1080\n",
    "rgbImg, depthImg, segImg, projection_matrix, view_matrix = get_camera_image('top_down', width=width, height=height, return_cam_parameters=True)\n",
    "rgbImg = rgba2rgb(rgbImg)\n",
    "\n",
    "#Get the camera intrinsics and extrinsics\n",
    "fx, fy, cx, cy = get_instrinsics_from_projection_matrix(projection_matrix, width, height)\n",
    "camera_intrinsics = CameraIntrinsics(fx, fy, cx, cy, width, height)\n",
    "\n",
    "#generate the point cloud for the mug\n",
    "mug_mask = segImg == 4\n",
    "mug_img = rgbImg.copy() * mug_mask.reshape(height, width, 1).repeat(3, axis=2)\n",
    "mug_depth = depthImg.copy() *mug_mask\n",
    "mug_pcd = o3d_create_pointcloud(mug_img, mug_depth, mug_mask, camera_intrinsics)\n",
    "\n",
    "#convert the point cloud from camera coordinate system to world coordinate system\n",
    "mug_pcd_t = deepcopy(mug_pcd).transform(np.linalg.inv(np.array(view_matrix).reshape(4,4)).T @ Tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the camera image from front perspective view\n",
    "width= 1080\n",
    "height = 1080\n",
    "rgbImg_front, depthImg_front, segImg_front, projection_matrix_front, view_matrix_front = get_camera_image('front_perspective', width=width, height=height, return_cam_parameters=True)\n",
    "rgbImg_front = rgba2rgb(rgbImg_front)\n",
    "\n",
    "#Get the camera intrinsics and extrinsics\n",
    "fx, fy, cx, cy = get_instrinsics_from_projection_matrix(projection_matrix_front, width, height)\n",
    "camera_intrinsics_front = CameraIntrinsics(fx, fy, cx, cy, width, height)\n",
    "\n",
    "mug_mask_front = segImg_front == 4\n",
    "mug_img_front = rgbImg_front.copy() * mug_mask_front.reshape(height, width, 1).repeat(3, axis=2)\n",
    "mug_depth_front = depthImg_front.copy() * mug_mask_front\n",
    "mug_pcd_front = o3d_create_pointcloud(mug_img_front, mug_depth_front, mug_mask_front, camera_intrinsics_front)\n",
    "\n",
    "#convert the point cloud from camera coordinate system to world coordinate system\n",
    "mug_pcd_front_t = deepcopy(mug_pcd_front).transform(np.linalg.inv(np.array(view_matrix_front).reshape(4,4)).T @ Tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pointcloud([mug_pcd_t, mug_pcd_front_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the specified directory to the pybullet search path\n",
    "p.setAdditionalSearchPath(\"/home/coding-cheetah/Research/simulator/envs/bullet/assets\")\n",
    "\n",
    "# Load the object from the specified directory\n",
    "object_position = [-0.3, 0.7, 0.63]\n",
    "object_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "\n",
    "# Load the texture\n",
    "texture_id = p.loadTexture(\"ycb/001_chips_can/tsdf/textured.png\")\n",
    "\n",
    "# Create a visual shape for the object with texture\n",
    "visual_shape_id = p.createVisualShape(\n",
    "    shapeType=p.GEOM_MESH,\n",
    "    fileName=\"ycb/001_chips_can/tsdf/textured.obj\",\n",
    "    meshScale=[1, 1, 1],\n",
    "    textureUniqueId=texture_id\n",
    ")\n",
    "\n",
    "# Create a multi-body for the object\n",
    "object_id = p.createMultiBody(\n",
    "    baseMass=1,\n",
    "    baseInertialFramePosition=[0, 0, 0],\n",
    "    baseVisualShapeIndex=visual_shape_id,\n",
    "    basePosition=object_position,\n",
    "    baseOrientation=object_orientation\n",
    ")\n",
    "\n",
    "# Print the object ID to confirm it has been loaded\n",
    "print(\"The object ID is: \", object_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    p.stepSimulation()\n",
    "    time.sleep(1/240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.setAdditionalSearchPath(\"/home/coding-cheetah/Research/simulator/envs/bullet/assets\")\n",
    "p.loadURDF(\"ycb/001_chips_can/tsdf/chips_can.urdf\", [0, 0.2, 0.63], p.getQuaternionFromEuler([0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.setAdditionalSearchPath(\"/home/coding-cheetah/Research/simulator/envs/bullet/assets\")\n",
    "objects = p.loadSDF(\"gso/5_HTP/model.sdf\", globalScaling = 3)\n",
    "position = [0.2, 0.3, 0.64]\n",
    "orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "p.resetBasePositionAndOrientation(objects[0], position, orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.setAdditionalSearchPath(\"/home/coding-cheetah/Research/simulator/envs/bullet/assets\")\n",
    "object = p.loadSDF('ycb/011_banana/google_512k/model.sdf' )\n",
    "position = [0.3, 0.3, 0.63]\n",
    "orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "p.resetBasePositionAndOrientation(object[0], position, orientation)\n",
    "# texture_id = p.loadTexture(\"ycb/024_bowl/google_512k/texture_map.png\")\n",
    "# p.changeVisualShape(object[0], -1, textureUniqueId=texture_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_position = [0, 0, 1.5]  # Change this to position the light source\n",
    "p.addUserDebugLine([0, 0, 0], light_position, [1, 1, 1], lineWidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.addUserDebugLine([0, 0, 0], [5, 5, 5], [1, 1, 1], lineWidth=2)\n",
    "p.addUserDebugLine([0, 0, 0], [-5, -5, 5], [1, 1, 1], lineWidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.setAdditionalSearchPath(\"/home/coding-cheetah/Research/simulator/envs/bullet/assets\")\n",
    "# Load the reference object using SDF\n",
    "reference_objects = p.loadSDF(\"ycb/024_bowl/google_512k/model.sdf\")\n",
    "reference_object = reference_objects[0]  # Assuming only one object in the SDF\n",
    "\n",
    "# Set initial position and orientation for the reference object\n",
    "ref_position = [0, 0, 0.64]  # Reference object at the origin\n",
    "ref_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "p.resetBasePositionAndOrientation(reference_object, ref_position, ref_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Get the bounding box of the reference object\n",
    "aabb_min, aabb_max = p.getAABB(reference_object)\n",
    "ref_dim = np.array(aabb_max) - np.array(aabb_min)  # Dimensions of the reference object\n",
    "ref_center = np.array([(aabb_min[i] + aabb_max[i]) / 2 for i in range(3)])  # Center position\n",
    "\n",
    "# Calculate the left region based on dimensions\n",
    "left_region_min = [aabb_min[0] - ref_dim[0], aabb_min[1], aabb_min[2]]\n",
    "left_region_max = [aabb_min[0], aabb_max[1], aabb_max[2]]\n",
    "\n",
    "# 3. Sample a position and orientation in the left region\n",
    "def sample_position_and_orientation(region_min, region_max):\n",
    "    sampled_position = [\n",
    "        np.random.uniform(region_min[0], region_max[0]),\n",
    "        np.random.uniform(region_min[1], region_max[1]),\n",
    "        np.random.uniform(region_min[2], region_max[2]),\n",
    "    ]\n",
    "    sampled_orientation = p.getQuaternionFromEuler(\n",
    "        [np.random.uniform(-np.pi, np.pi) for _ in range(3)]\n",
    "    )\n",
    "    return sampled_position, sampled_orientation\n",
    "\n",
    "# 4. Check for collision\n",
    "def is_colliding(object_id, position, orientation):\n",
    "    # Temporarily move the object to check for collisions\n",
    "    temp_objs = p.loadSDF(\"ycb/011_banana/google_512k/model.sdf\")\n",
    "    temp_obj = temp_objs[0]\n",
    "    p.resetBasePositionAndOrientation(temp_obj, position, orientation)\n",
    "    for _ in range(1000):\n",
    "        p.stepSimulation()\n",
    "        time.sleep(1/240)\n",
    "    contact_points = p.getClosestPoints(temp_obj, reference_object, distance=0.0)\n",
    "    p.removeBody(temp_obj)\n",
    "    return len(contact_points) > 0\n",
    "\n",
    "# 5. Place the target object\n",
    "target_placed = False\n",
    "for _ in range(100):  # Try 100 samples\n",
    "    target_position, target_orientation = sample_position_and_orientation(left_region_min, left_region_max)\n",
    "    if not is_colliding(reference_object, target_position, target_orientation):\n",
    "        # Successfully found a valid position\n",
    "        target_objs = p.loadSDF(\"ycb/011_banana/google_512k/model.sdf\")\n",
    "        p.resetBasePositionAndOrientation(target_objs[0], target_position, target_orientation)\n",
    "        target_placed = True\n",
    "        print(\"Target object placed at:\", target_position)\n",
    "        break\n",
    "\n",
    "if not target_placed:\n",
    "    print(\"Failed to place the target object without collision.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
